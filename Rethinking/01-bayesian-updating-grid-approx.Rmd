---
title: "Updating and Grid Approximation"
author: "STAT 341, SP23"
date: "2023-01-13"
output: 
  html_document:
    toc: true
    toc_float: true
    code_download: true
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggformula)
theme_set(theme_minimal(base_size = 14))
knitr::opts_chunk$set(echo = TRUE)
```

# Recreating SR Figure 2.5

What if, instead of just 4 options for a "conjecture" about the proportion blood in the body, we considered a range of values from 0-1?

Let's draw a picture of how each new observation will update our posterior answer, if we begin thinking that *all* values between 0-1 are equally likely. (That's *not* a good or realistic prior, but we will use it this once for simplicity.)

**The code below creates the desired figure for the "water and land on the globe" example in SR Chapter 2. You will need to make changes to make it represent our "blood in the body" example!**

Assume B is blood (a "success" at getting the thing we're trying to measure the probability of), and F is not-blood, and we got the data:

**F B F F B F F F B**

### Create a data table (`tibble`) containing the observed data

```{r}
# in SR, w is water  and l is land, and w is "success"
blood_data <- tibble(obs = c("F", "B", "F", "F", "B", "F", "F", "F", "B"))
```

### Add variables recording the number of datapoints and total "successes" so far

```{r}
blood_data  <- blood_data |>
  # mutate() adds a new variable to a dataset
  mutate(n_trials = c(1:9),
         # cumsum() computes the cumulative sum of a vector
         # == tests whether the left and right-hand objects are equal and returns 0 for no, 1 for yes
         n_success = cumsum(obs == 'B'))
```

### Calculations to prepare for plotting

```{r}
sequence_length <- 50

blood_data <- blood_data |> 
  tidyr::expand(nesting(n_trials, obs, n_success), 
         # don't forget to  change variable names to match our problem!
         p_blood = seq(from = 0, to = 1, length.out = sequence_length)) 

# pause here and see what your data table now looks like
glimpse(blood_data)
```

```{r}
blood_data <- blood_data  |> 
  group_by(p_blood) |> 
  mutate(lagged_n_trials  = dplyr::lag(n_trials, n = 1),
         lagged_n_success = dplyr::lag(n_success, n = 1)) |> 
  ungroup() |> 
  mutate(prior      = ifelse(n_trials == 1, 
                             # if it's the first trial then:
                             .5,
                             # otherwise on later trials the prior will be:
                             dbinom(x    = lagged_n_success, 
                                    size = lagged_n_trials, 
                                    prob = p_blood)),
         likelihood = dbinom(x    = n_success, 
                             size = n_trials, 
                             prob = p_blood),
         #  "strip" is going to be used to make a title for  the graph panel
         strip      = str_c("n = ", n_trials)) |> 
  # the next three lines allow us to normalize the prior and the likelihood, 
  # ensuring the sum over all probabilities is 1 for both of them
  group_by(n_trials) |> 
  mutate(prior      = prior / sum(prior),
         likelihood = likelihood / sum(likelihood))  
```

### Make the plot!

```{r}
gf_line(likelihood ~ p_blood,
        data  = blood_data) |>
  gf_line(prior ~ p_blood,
          data = blood_data,
          linetype = 'dashed') |>
  gf_facet_wrap(~ strip,
                scales = "free_y")
```

### Questions

1.  What is the `sequence_length` controlling? What happens to the graph if you make it too small? (Too big?)

    > The `sequence_length` is controlling the "smoothness" of the line and resolution of the grid? The line is more precise if `sequence_length` is higher.

2.  What is the dotted line in each panel?

    > Prior.

3.  What is the solid line in each panel?

    > Likelihood.

# Grid Approximation

### What about the Posterior?

The example above gives insight into how each added data-point *updates* our knowledge, but it skims over exactly how we can compute the posterior for a given sample by multiplying the **prior** $\times$ likelihood, and how we could obtain the final posterior for our whole dataset.

What we've done so far is a **grid approximation**, where we find the posterior for some finite set of possible values of $p$ that are evenly spaced over its possible values (here: 0-1).

In class Wednesday we had *very* few values (just 6: 0/5, 1/5, 2/5, 3/5, 4/5, and 5/5).

### Make the Calculations

With the help of R, let's try a finer grid to get a more precise answer...

**Again, this code is for the SR book's "land and water" example. Make all changes needed to make it relevant to our "bloody" example!**

```{r}
blood_data2 <-
  tibble(p_grid = seq(from = 0, to = 1, length.out = 20),      # define grid
         prior  = 1) |>                                       # define prior
  mutate(likelihood = dbinom(3, size = 9, prob = p_grid)) |>  # compute likelihood at each value in grid
  mutate(unstd_posterior = likelihood * prior) |>             # compute product of likelihood and prior
  mutate(posterior = unstd_posterior / sum(unstd_posterior))   # standardize the posterior, so it sums to 1


# to peek at the results table
glimpse(blood_data2)
```

We can draw our plot with less code if, instead of having the prior, likelihood, and posterior densities in different columns, we stack them on top of each other and add a new column to label whether each case is from the prior, posterior, or likelihood:

```{r}
blood_data2 <- blood_data2 |>
  # remove the unstandardized posterior (don't need it anymore)
  select(-unstd_posterior) |>
  pivot_longer(prior:posterior)

glimpse(blood_data2)
```

### Draw the plot

```{r}
gf_line(value ~ p_grid,
        color = ~ name,
        data =  blood_data2) |>
  gf_point() |>
  gf_facet_wrap(~name,
                scales = "free_y")
```

### Questions

1.  What happens if you make the number of conjectured values of $p$ (the fine-ness of the grid) smaller, or larger? Re-run your code a few times to see what happens with very different grid resolutions.

    > When p is higher, the posterior is more precise and smooth.

2.  (*relies on previous R experience*) What if you use a different prior - how do results change?

    > When the prior is strong, it could affect the posterior more than the data, if the prior is uniform, the posterior might rely more on the data.

3.  (*advanced/tricky*) Why are the posterior, the prior and the likelihood in our plot different heights? Why doesn't that bother Prof DR a lot?

    > Prior is our knowledge before seeing the data, likelihood is observed data, posterior is combining the prior and likelihood. We are not expecting them to have same heights.

4.  What happens if you use a smaller dataset (just the initial observations instead of all of them)? A larger one?

    > When the dataset is larger, we are more confident in the data than in our prior, so the dataset influences the posterior more.
    >
    > When the dataset is smaller, we are less confident in the data than in our prior, so the dataset influences the posterior less.

### What's next?

The grid approach is simple, but really limited: the grid resolution you'd need to get the precision you want in your posterior estimate might crash your computer, for one thing. (Another way of putting it is that grid search is slow and inefficient.) We will learn more efficient ways, soon!

*Note: thanks to <https://bookdown.org/content/4857/> for base for the code used here.*
