---
title: "Linear Model Practice"
author: "STAT 341, Feb. 13, 2023"
date: ""
output:
  html_document:
    toc: true
    toc_float: true
    code_download: true
  pdf_document: default
---

```{r setup, include=FALSE, message=FALSE}
library(faraway)
library(tidyverse)
library(rethinking)
library(CalvinBayes)
knitr::opts_chunk$set(echo = TRUE)
```

## Instructions
Choose one or more of the options below to practice linear regression. Save your group's work in an Rmd file (that you may later present  to the class). The process:

- Choose a response variable and a single main predictor. Have a  rationale for your choices (even if it's not written down in detail).
- Sketch (by pencil or using `dagitty`) a **causal diagram** for this situation. You may want to include other variables from the dataset, or even ones that you *don't* have data  on. By using just one predictor and the response, what other key covariates might you be ignoring (if any)?
- Write out a **description of a model** including your predictor and response variables (only). Use a normal distribution to model your response (your choice of distribution(s) for priors). If needed, use internet research to get background and select priors.
- Now you can peek at the data...consider making a scatter plot of the response vs the predictor. (`gf_point(response ~ predictor, data = dataset_name)` will do it for you.)
- Do a **prior predictive check** (generate a  prior predictive distribution as a reality check for  your priors)
- Use `quap()` to fit your model.
- Think about how you might graph your **posterior distributions.** You can at least make a  density plot or two. But...what do those mean? Are they easy to interpret? What is *really* most relevant to show? Consider carefully and sketch what  you'd *want* to show. If you are bold and confident with your R skills, you can consider also making the graphs you envision. (Stay tuned for Wednesday.)
- Maybe, for extra challenge: Generate and graph a **posterior predictive distribution**, comparing it with your observed data.
- Considering all you have done...what can you conclude?

## Dataset Options
### Learning Styles

A popular idea in education research was that different students have different learning styles (visual, auditory, kinetic, etc.) and that they learn best when the teaching method matches their preferred style. The idea makes intuitive sense, but little to no empirical evidence has supported it. The paper [Matching Learning Style to Instructional Method: Effects on Comprehension](https://www.apa.org/pubs/journals/features/edu-a0037478.pdf) investiaged.

You can get (a simulated version of) some of the data from the paper at: <https://sldr.netlify.app/data/learning-styles.csv>.  In the dataset, `auditory_style` and `visual_style` are higher when a person more strongly prefers an aural or visual learning style.  (`style_diff` is the difference between the two.)  `listening_aptitude` and `reading_aptitude` are scores designed to measure students' actual learning after aural (listening) and visual (reading) lessons (and `aptitude_diff` is the difference between the two scores).

**Can you replicate any of the paper's findings?**


### Texas Highways 
What is the relationship between rainfall volume  and 
runoff volume (both in $m^3$) for a particular stretch of a Texas highway? The code below accesses and tidies up a relevant dataset.
	
```{r, echo=TRUE, message=FALSE}
library(Devore7)
TexasHighway <- ex12.16
names(TexasHighway) <- c('rainfall','runoff')
```


### Bike Lanes
Some traffic engineers were interested to study interactions between bicycle and   automobile traffic.  One part of the study involved comparing the amount of "available space'' for a bicyclist 	(distance in feet from bicycle to centerline of the roadway) and 	"separation distance'' (the average distance between cyclists and passing car, also measured in feet, 	determined by averaging based on photography over an extend period of time).	Data were collected at 10 different sites with bicycle lanes.  

```{r, bikedata_in, echo=TRUE}
bikedat <- read.csv("https://sldr.netlify.app/data/bikedata.csv")
```

### Cake! (Chocolate, of course.) 
An industrious grad student studied the effect of different recipes and baking conditions on the breaking angle of cake (this is the angle to which a slice can be bent before it breaks in two). The data are in package `faraway`.

```{r }
library(faraway)
head(choccake)
```

### Cheese! (Cheddar, of course.) 
How do chemical properties of cheese affect its taste score? Explore this exciting question with the dataset `cheddar` from package `faraway`.

```{r }
library(faraway)
head(cheddar)
```

### Fat. 
How does men's percent body fat depend on different body measurements? Explore with the dataset `fat` from package `faraway`.  

```{r, }
library(faraway)
head(fat)
# for more info, type in the R console:
# ?fat
```

### Bridges
This question uses data on bridges from the New York State Department of Transportation.  The data set description reads: ``New York State inspectors assess all of the bridges every two years including a bridge's individual parts. Bridges are analyzed for their capacity to carry vehicular loads. Inspectors are required to evaluate, assign a condition score, and document the condition of up to 47 structural elements, including rating 25 components of each span of a bridge, in addition to general components common to all bridges. The NYSDOT condition rating scale ranges from 1 to 7, with 7 being in new condition and a rating of 5 or greater considered as good condition.

```{r, bridge_in}
bridges <- read.csv("https://sldr.netlify.app/data/bridges.csv")
```

Remove bridges with Condition.Rating = 0 (closed bridges) from the data set before beginning:

```{r, zero_out}
bridges <- bridges |> filter(Condition.Rating >= 1)
```


If you look at these data you'll notice that some bridges were built in the future (the data were collected in 2015) -- oops!
Let's remove those too.

```{r }
bridges <- bridges |> filter(Year.Built <= 2015)
```

### Dinos
This question uses data on dinosaurs! The data are from a paper by Nicolas Campione and David Evans entitled ``A universal scaling relationship between body mass and proximal limb bone dimensions in quadrupedal terrestrial tetrapods", published in 2012 in BMC biology.  The paper is available online at \url{http://www.biomedcentral.com/1741-7007/10/60} if you are interested.  Basically, the authors were able to predict the body mass of dinosaurs based on the dimensions of their bones (note: some of the models they used were more sophisticated than the ones we'll consider below, but we can get close to what they did).  To make their predictions, the authors used data on bone sizes and body masses of many non-extinct animals.

```{r, dinos_in}
dinomass <- read.csv("https://sldr.netlify.app/data/Campione_Evans_2012_data.csv")
```


### Primate Brains 



Powell and colleagues recently published [a study of predictors of primate brain size](https://royalsocietypublishing.org/doi/10.1098/rspb.2017.1765#d3e1601). We will examine some of the data they used, available at [https://sldr.netlify.app/data/primates.csv](https://sldr.netlify.app/data/primates.csv).  Variables include:

- `Species`: The primate species name
- `ECV`: endocranial volume (a measure of brain size)
- `Diurnal`: whether the species is active diurnally (in the daytime) or not
- `Habitat`: whether the species is Terrestrial (ground living) or Arborial (tree living)
- `BodyMass`: Average body mass (in grams) of individuals of this species
- `HomeRangeSize`: Size (in square kilometers) of the home area of the species
- `GroupSize`: Average group size for the species (number of individuals)

*Note: in this dataset, the actual relationship(s) are not linear. Can you find a model that  is a  good match to the data?*

### West Nile Virus

[Paull and colleagues](https://doi.org/10.1098/rspb.2016.2078) recently published a study on the spread of West Nile virus in relation to climate and climate change. One part of the study examined the number of human cases of West Nile virus in Colorado. The dataset is at: [https://sldr.netlify.app/data/COWestNile.csv](https://sldr.netlify.app/data/COWestNile.csv) and contains variables:

- `HumanCases`: The number of human West Nile Virus cases in a certain Colorado county in a given week
- `MosquitoIndex`: A measure of infected mosquito abundance for a certain Colorado county and week
- `FirstExposureYear`: Whether or not it was the first year West Nile occurred in CO (2003). `Yes` means it was 2003, the first year; `No` means it was not.

The authors of the paper used regression to see how the number of human cases depended on the infected mosquito abundance, and whether the population was naive (whether or not it was the first year of West Nile presence). 

*Note: in this dataset, the actual relationship(s) are not linear. Can you find a model that  is a  good match to the data?*
